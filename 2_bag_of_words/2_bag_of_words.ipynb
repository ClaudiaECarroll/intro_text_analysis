{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Code Demo Two for Text Analysis with Python, covering:\n",
        "\n",
        "* Review of loading and cleaning text data\n",
        "* Lexicon-based sentiment analysis\n",
        "* Part-of-Speech (POS) Tagging\n",
        "* Named Entity Recognition (NER)"
      ],
      "metadata": {
        "id": "KrkElyCQhJN4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Text for Bag-of-Words Analysis"
      ],
      "metadata": {
        "id": "EPKMlOSrsC1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading in the text data from the text file\n",
        "\n",
        "import os\n",
        "\n",
        "grimms_folder = \"/content/drive/MyDrive/TRIADS_workshops/grimms\"\n",
        "\n",
        "rapunzel = open(os.path.join(grimms_folder, \"rapunzel.txt\"), encoding=\"utf-8\").read()\n",
        "\n"
      ],
      "metadata": {
        "id": "Gvus3So5Iajj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rapunzel)"
      ],
      "metadata": {
        "id": "5tLQjZmgK6C1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing using NLTK\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokens = word_tokenize(rapunzel)\n",
        "\n",
        "print(tokens)\n",
        "\n"
      ],
      "metadata": {
        "id": "JBYqaHVW-LX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing stopwords\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords = stopwords.words(\"english\")\n",
        "\n",
        "tokens = [token for token in tokens if token not in stopwords]\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "poAzYifH5UKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning tokens of punctuation\n",
        "\n",
        "\n",
        "import string\n",
        "punctuation = list(string.punctuation)\n",
        "\n",
        "punctuation.append(\"‘\")\n",
        "punctuation.append(\"’\")\n",
        "\n",
        "tokens = [token for token in tokens if token not in punctuation]\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "0j8539aX4-H0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "id": "3JmvUPMhNDHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making tokens all lowercase\n",
        "\n",
        "tokens = [token.lower() for token in tokens]\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "Qs5I4ZvurkUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo: Sentiment Analysis--Lexicon/Dictionary Approach"
      ],
      "metadata": {
        "id": "9HGfc9GjyRvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import sentiment analyzer tool from NLTK\n",
        "\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Download the VADER lexicon\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "#Loading the VADER sentiment analyzer tool\n",
        "\n",
        "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
        "\n"
      ],
      "metadata": {
        "id": "5B_0TlIUx11A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Assigning the VADER lexicon to a variable and taking a look at the lexicon\n",
        "\n",
        "vader_lexicon = list(sentiment_analyzer.lexicon.items())\n",
        "\n",
        "print(vader_lexicon[2000:2050])\n"
      ],
      "metadata": {
        "id": "_fX_jZn2yqs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating sentiment scores for Rapunzel\n",
        "\n",
        "sentiment_scores = sentiment_analyzer.polarity_scores(rapunzel)\n",
        "print(sentiment_scores)"
      ],
      "metadata": {
        "id": "f72IlSwoyqmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting just the overall sentiment scores\n",
        "\n",
        "sentiment_scores[\"compound\"]"
      ],
      "metadata": {
        "id": "L0-T8SKq03qX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise: Sentiment Scores\n",
        "\n",
        "1. Calculate the negative sentiment score for Hansel and Gretal\n",
        "\n",
        "2. Calculate the compound sentiment score for each story in Grimm's Fairytales by looping through the files in your Grimms folder.\n",
        "\n"
      ],
      "metadata": {
        "id": "4p-tUDuvf8pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the negative sentiment score for Hansel and Gretal\n",
        "\n",
        "hansel_gretel = open(os.path.join(grimms_folder, \"hansel_and_gretel.txt\"), encoding=\"utf-8\").read()\n",
        "\n",
        "sentiment_scores_HG = sentiment_analyzer.polarity_scores(hansel_gretel)\n",
        "\n",
        "sentiment_scores_HG[\"neg\"]\n"
      ],
      "metadata": {
        "id": "kaGX0sFizDhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating sentiment scores for ALL text files in a folder\n",
        "\n",
        "results = []\n",
        "\n",
        "for file in os.listdir(grimms_folder):\n",
        "  if file[-4:] == \".txt\":\n",
        "\n",
        "    file_name = open(os.path.join(grimms_folder, file), encoding=\"utf-8\")\n",
        "    text = file_name.read()\n",
        "\n",
        "    sentiment_scores = sentiment_analyzer.polarity_scores(text)\n",
        "    compound_score = sentiment_scores[\"compound\"]\n",
        "    results.append([file, compound_score])\n",
        "\n",
        "print(results)"
      ],
      "metadata": {
        "id": "dFdKlF3k1gJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demo: Part-of-Speech (POS) Tagging"
      ],
      "metadata": {
        "id": "l1X2Nhr6_DqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import sentence tokenizer from NLTK\n",
        "# You must tokenize by sentence for POS tagging, as the tool uses the sentence structure to determin the POS\n",
        "\n",
        "from nltk import sent_tokenize\n",
        "\n",
        "rapunzel_sentences = sent_tokenize(rapunzel)\n",
        "rapunzel_sentences[:2]"
      ],
      "metadata": {
        "id": "tlAgeVhr_0CO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use .replace() to remove new line characters (\\n) from sentences\n",
        "\n",
        "rapunzel_sentences_cleaned = []\n",
        "\n",
        "for x in rapunzel_sentences:\n",
        "  x = x.replace(\"\\n\", \" \")\n",
        "  rapunzel_sentences_cleaned.append(x)\n",
        "\n",
        "rapunzel_sentences = rapunzel_sentences_cleaned\n",
        "\n",
        "\n",
        "rapunzel_sentences[:2]"
      ],
      "metadata": {
        "id": "czGxAmgxA6YH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing to POS tag ONE of the sentences\n",
        "\n",
        "example_sentence = rapunzel_sentences[0]\n",
        "\n",
        "words_example = nltk.word_tokenize(example_sentence)\n",
        "\n",
        "print(words_example)\n",
        "\n"
      ],
      "metadata": {
        "id": "egi_67EQu4Z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# POS tagging our example sentence\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "pos_example = nltk.pos_tag(words_example)\n",
        "\n",
        "print(pos_example)"
      ],
      "metadata": {
        "id": "XdhH-wYMvLUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# POS tagging ALL the sentences in Rapunzel\n",
        "\n",
        "pos = []\n",
        "\n",
        "for sentence in rapunzel_sentences:\n",
        "  words = nltk.word_tokenize(sentence)\n",
        "  pos_tags = nltk.pos_tag(words)\n",
        "  pos.append(pos_tags)\n",
        "\n",
        "pos[:5]"
      ],
      "metadata": {
        "id": "vLJ3TSNk_nrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting each part of speech in sentence in Rapunzel by looping through each word/tag pair in each sentence\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "pos_counts_per_sentence = []\n",
        "\n",
        "for tagged_sentence in pos:\n",
        "    tags = []\n",
        "    for word, tag in tagged_sentence:\n",
        "        tags.append(tag)\n",
        "    counts = Counter(tags)\n",
        "    pos_counts_per_sentence.append(counts)\n",
        "\n",
        "\n",
        "print(pos_counts_per_sentence)\n"
      ],
      "metadata": {
        "id": "6kFwRU1XwEGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating just the number of VERBS in each sentence of Rapunzel\n",
        "\n",
        "verb_totals = []\n",
        "\n",
        "for tagged_sentence in pos:\n",
        "    verb_count = 0  # start counter for this sentence\n",
        "    for word, tag in tagged_sentence:\n",
        "        if tag.startswith(\"VB\"):  # check if it's a verb tag\n",
        "            verb_count += 1       # add 1 for each verb\n",
        "    verb_totals.append(verb_count)\n",
        "\n",
        "print(verb_totals)"
      ],
      "metadata": {
        "id": "q37kKn6XBqcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an overall count of each POS tag in Rapunzel by aggregating the POS counts for each sentence\n",
        "\n",
        "\n",
        "total_counts = Counter()\n",
        "\n",
        "for counts in pos_counts_per_sentence:\n",
        "    total_counts.update(counts)\n",
        "\n",
        "print(total_counts)\n"
      ],
      "metadata": {
        "id": "Y0hXl1wZwD9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing our overall POS tag data to a dataframe\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "pos_total_df = pd.DataFrame(total_counts.items(), columns=['POS', 'Count'])\n",
        "\n",
        "pos_total_df = pos_total_df.sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
        "\n",
        "pos_total_df"
      ],
      "metadata": {
        "id": "tIphsdJhxtY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving our results to a CSV!\n",
        "\n",
        "pos_total_df.to_csv(\"rapunzel_pos_data.csv\", header=True, index=False)"
      ],
      "metadata": {
        "id": "-oLFPVDbyRi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise: POS Tagging\n",
        "\n",
        "1.   Calculate the total number of nouns in The Frog Prince. (Try writing down the steps you need to follow on paper first!)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pvUUe6RlAuZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in the text data\n",
        "\n",
        "frog = open(os.path.join(grimms_folder, \"the_frog-prince.txt\"), encoding=\"utf-8\").read()\n",
        "\n",
        "# Tokenize by sentence\n",
        "\n",
        "frog_sentences = sent_tokenize(frog)\n",
        "\n",
        "# Word tokenize and POS tag each sentence\n",
        "\n",
        "frog_pos = []\n",
        "\n",
        "for sentence in frog_sentences:\n",
        "  words = nltk.word_tokenize(sentence)\n",
        "  pos_tags = nltk.pos_tag(words)\n",
        "  frog_pos.append(pos_tags)\n",
        "\n",
        "# Loop through the pos tags for each sentence and count each instance of a verb\n",
        "\n",
        "frog_noun_totals = []\n",
        "\n",
        "for tagged_sentence in frog_pos:\n",
        "    noun_count = 0\n",
        "    for word, tag in tagged_sentence:\n",
        "        if tag.startswith(\"NN\"):\n",
        "            noun_count += 1\n",
        "    frog_noun_totals.append(noun_count)\n",
        "\n",
        "# Sum together all of the noun counts\n",
        "\n",
        "frog_aggregate = sum(frog_noun_totals)\n",
        "\n",
        "print(frog_aggregate)"
      ],
      "metadata": {
        "id": "p-sIeQHcCG4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demo: Names Entity Recognition"
      ],
      "metadata": {
        "id": "MzUJxw6wv3ay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading one sentence to use as an example\n",
        "\n",
        "one_sentence = rapunzel_sentences[1]\n",
        "print(one_sentence)"
      ],
      "metadata": {
        "id": "AEI7HDpNFKmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the necessary nltk packages, and running the NER tool on our example sentence\n",
        "\n",
        "from nltk import ne_chunk\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('words')\n",
        "\n",
        "# Step 1: tokenize the sentence\n",
        "x = word_tokenize(one_sentence)\n",
        "\n",
        "#Step 2: POS tag the sentence\n",
        "y = nltk.pos_tag(x)\n",
        "\n",
        "# Step 3: Apply NER to the pos tags\n",
        "\n",
        "ner = ne_chunk(y)"
      ],
      "metadata": {
        "id": "hIbIaa6xFTbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets take a look at what we end up with:\n",
        "\n",
        "print(ner)"
      ],
      "metadata": {
        "id": "bxijU8PJKgnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What the heck is that? Let's use type() to find out...\n",
        "\n",
        "type(ner)"
      ],
      "metadata": {
        "id": "zU-1AJwBHN3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use svgling package to visualize the POS and NER tree\n",
        "\n",
        "!pip install svgling\n",
        "import svgling\n",
        "\n",
        "tree_svg = svgling.draw_tree(ner)\n",
        "display(tree_svg)"
      ],
      "metadata": {
        "id": "4PAhK5UkHNvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract out only the subtree with a Named Entity in it\n",
        "\n",
        "subtree = ner[6]\n",
        "print(subtree)"
      ],
      "metadata": {
        "id": "gIpZgKbCmHw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print that Named Entity and it's POS tag\n",
        "\n",
        "print(subtree.leaves())"
      ],
      "metadata": {
        "id": "Dfvb7y3iKV_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets get just the Named Entity\n",
        "\n",
        "entity_name = subtree.leaves()[0][0]\n",
        "print(entity_name)"
      ],
      "metadata": {
        "id": "e9rOQ5JQJEJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# And now lets just get the label, or type of Named Entity it is\n",
        "\n",
        "entity_type = subtree.label()\n",
        "print(entity_type)"
      ],
      "metadata": {
        "id": "8gdkwXramHrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run NER on every sentence in Rapunzel!!\n",
        "\n",
        "ner_sentences = []\n",
        "\n",
        "for sentence in rapunzel_sentences:\n",
        "    words = word_tokenize(sentence)\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "    ner_tree = ne_chunk(pos_tags)\n",
        "    ner_sentences.append(ner_tree)\n"
      ],
      "metadata": {
        "id": "demKoafckkIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets visualize one of the sentences and the results\n",
        "\n",
        "ner_sentences[24]"
      ],
      "metadata": {
        "id": "pptej1ECIRk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of named entities, for each entity type in Rapunzel\n",
        "\n",
        "from nltk import Tree\n",
        "\n",
        "named_entities = {}\n",
        "\n",
        "for sentence in ner_sentences:\n",
        "  for part in sentence:\n",
        "    if type(part) == Tree:\n",
        "          entity_name = part.leaves()[0][0]\n",
        "          entity_type = part.label()\n",
        "\n",
        "          if entity_type in named_entities:\n",
        "            named_entities[entity_type].append(entity_name)\n",
        "          else:\n",
        "            named_entities[entity_type] = [entity_name]\n",
        "\n",
        "\n",
        "print(\"Named Entities:\", named_entities)"
      ],
      "metadata": {
        "id": "9L5UQAVcJvoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate now many times each PERSON named entity appears in Rapunzel\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "person_counts = Counter(named_entities.get('PERSON', []))\n",
        "\n",
        "# Display the result\n",
        "print(\"Person Counts:\", person_counts)"
      ],
      "metadata": {
        "id": "0Q-SHsOTpkXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets get each unique Named Entity, it's type, and how many times is appears in the text\n",
        "\n",
        "entities = []\n",
        "types = []\n",
        "counts = []\n",
        "\n",
        "for entity_type in named_entities:\n",
        "    names = named_entities[entity_type]\n",
        "    name_counts = Counter(names)\n",
        "    for name in name_counts:\n",
        "        entities.append(name)\n",
        "        types.append(entity_type)\n",
        "        counts.append(name_counts[name])\n",
        "\n",
        "print(entities)\n",
        "print(types)\n",
        "print(counts)\n"
      ],
      "metadata": {
        "id": "4ky2xJ4RrAyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save our entity data to a dataframe\n",
        "\n",
        "#Create a dataframe with column names and data from prior cell\n",
        "entities_df = pd.DataFrame({\n",
        "    'Entity': entities,\n",
        "    'Type': types,\n",
        "    'Count': counts})\n",
        "\n",
        "# Sort the dataframe by the most frequent entity\n",
        "entities_df = entities_df.sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
        "\n",
        "entities_df"
      ],
      "metadata": {
        "id": "GcUQd0Hbur4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save our entity data to a CSV!\n",
        "\n",
        "entities_df.to_csv(\"rapunzel_NER.csv\", header=True, index=False)"
      ],
      "metadata": {
        "id": "7s-buKloNhVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Exercise: Named Entity Recognition\n",
        "\n",
        "\n",
        "\n",
        "1.  Named Entity Recognition requires three pre-processing steps. List them.\n",
        "2.  Make a list of the Named Entities in Snow White (\"snow-white_and_rose-red.txt\"). Do not include the entity types, and your list should not have any repeat values.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0fOYVxa-u0PW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Pre-Processing Steps for Named Entity Recognition:\n",
        "\n",
        "1.   Tokenize by sentence\n",
        "2.   Tokenize each sentence by word\n",
        "3.   Part-of-speech tag the words\n",
        "\n"
      ],
      "metadata": {
        "id": "iFAWzicHjqwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load text file and tokenize\n",
        "\n",
        "snow_white = open(os.path.join(grimms_folder, \"snow-white_and_rose-red.txt\"), encoding=\"utf-8\").read()\n",
        "\n",
        "snow_sentences = sent_tokenize(snow_white)\n",
        "\n",
        "# Apply NER to the sentences\n",
        "\n",
        "snow_ner_sentences = []\n",
        "\n",
        "for sentence in snow_sentences:\n",
        "    tokens = word_tokenize(sentence)\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    ner_tree = ne_chunk(pos_tags)\n",
        "    snow_ner_sentences.append(ner_tree)\n",
        "\n",
        "# Extract out the entities and save them to a list, skipping the entities we've already added\n",
        "\n",
        "entity_list = []\n",
        "\n",
        "for sentence in snow_ner_sentences:\n",
        "  for part in sentence:\n",
        "    if type(part) == Tree:\n",
        "          entity_name = part.leaves()[0][0]\n",
        "          if entity_name not in entity_list:\n",
        "              entity_list.append(entity_name)\n",
        "\n",
        "entity_list"
      ],
      "metadata": {
        "id": "DezoB9mcTjSk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}