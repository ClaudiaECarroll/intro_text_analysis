{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Code Demo One for Text Analysis with Python, covering:\n",
        "\n",
        "*   Loading text data\n",
        "*   Tokenization\n",
        "*   Text cleaning (incl. removing stopwords, puntuation and capital letters)\n",
        "* Basic word frequency analysis\n",
        "\n"
      ],
      "metadata": {
        "id": "m_B5Op9fgg30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "PAvHTfqi9Ixe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading and Processing Text Files"
      ],
      "metadata": {
        "id": "ER98tX-C5z7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grimms_folder = \"/content/drive/MyDrive/TRIADS_workshops/grimms\""
      ],
      "metadata": {
        "id": "RUoT-I3m9_dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "snow_white_file = open(os.path.join(grimms_folder, \"snow-white_and_rose-red.txt\"),\n",
        "                  encoding=\"utf-8\")\n",
        "\n",
        "snow_white = snow_white_file.read()"
      ],
      "metadata": {
        "id": "m0H8qYxaAGDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(snow_white)"
      ],
      "metadata": {
        "id": "KrpzfijgAINW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning and Pre-processing ##"
      ],
      "metadata": {
        "id": "_DtGzAsCCWiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing bibliographic text by splitting on the asterisks and keeping everything before\n",
        "\n",
        "snow_white_cleaned = snow_white.split(\"*****\")[0]\n"
      ],
      "metadata": {
        "id": "5QKoOqFtByn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# .split() breaks the text into parts on the designated string, and creates a list of strings out of the parts.\n",
        "\n",
        "snow_white.split(\"*****\")[1]"
      ],
      "metadata": {
        "id": "BWg5ZILicNg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printing the text cleaned of bibliographic data\n",
        "\n",
        "print(snow_white_cleaned)"
      ],
      "metadata": {
        "id": "2ka3U0KXB_PI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization"
      ],
      "metadata": {
        "id": "oKltDmVycqIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries (Natural Language TookKit (NLTK))\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n"
      ],
      "metadata": {
        "id": "XKwec5RPCKUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "\n",
        "tokens = word_tokenize(snow_white)\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "MIry3WjpC5o-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting the number of tokens in the text\n",
        "\n",
        "len(tokens)"
      ],
      "metadata": {
        "id": "3yqrMlouG1Yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Token Cleaning"
      ],
      "metadata": {
        "id": "XudcpuOtePj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a list of punctuation marks using the string library\n",
        "\n",
        "import string\n",
        "\n",
        "punctuation = list(string.punctuation)\n",
        "\n",
        "punctuation\n"
      ],
      "metadata": {
        "id": "a_oN6H4jDiz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding curly brackets to the punctuation list\n",
        "\n",
        "punctuation.extend([\"‘\", \"’\", \"“\", \"”\"])\n",
        "\n",
        "print(punctuation)"
      ],
      "metadata": {
        "id": "n9wXsziddLPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using a for loop to remove punctuation from the tokens list\n",
        "\n",
        "for token in tokens:\n",
        "  if token in punctuation:\n",
        "    tokens.remove(token)\n"
      ],
      "metadata": {
        "id": "chZRNpGJEKzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "id": "EgOke5YcG7EN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens)"
      ],
      "metadata": {
        "id": "MZ-9Zv9eEy0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternative way to remove punctuation--looping through the tokens list and keeping only alphanumeric tokens.\n",
        "\n",
        "filtered_tokens = []\n",
        "\n",
        "for token in tokens:\n",
        "    if token.isalpha():\n",
        "        filtered_tokens.append(token)\n",
        "\n",
        "tokens = filtered_tokens"
      ],
      "metadata": {
        "id": "orwTwEzbH5uD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "id": "oIt3xyqUeK3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternative way to keep only alpha-numeric tokens--using a list comprehension!\n",
        "\n",
        "tokens = [token for token in tokens if token.isalpha()]"
      ],
      "metadata": {
        "id": "oWHhFdPkeHGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making all tokens lowercase using list comprehension\n",
        "\n",
        "tokens = [token.lower() for token in tokens]\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "F4ueIe8OFHR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Word Frequency Stats"
      ],
      "metadata": {
        "id": "Nt891ODJeyF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate final word count\n",
        "\n",
        "word_count = len(tokens)\n",
        "print(word_count)"
      ],
      "metadata": {
        "id": "GJsO3PBPJB3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate most frequent words using Counter\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "word_counts = Counter(tokens)\n",
        "print(word_counts)"
      ],
      "metadata": {
        "id": "oTeG9uAYFThs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load stopword list from NLTK\n",
        "\n",
        "from nltk.corpus import stopwords # Load stopwords sub-library from NLTK\n",
        "\n",
        "nltk.download('stopwords') # Download stopword list (this is a quirk of NLTK)\n",
        "\n",
        "stopwords = set(stopwords.words('english'))\n",
        "\n",
        "print(stopwords)\n"
      ],
      "metadata": {
        "id": "5-ndJQgwFjlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using a list comprehension to loop through tokens and keep the tokens that are NOT in the stopword list\n",
        "\n",
        "content_words = [token for token in tokens if token not in stopwords]\n",
        "\n",
        "print(content_words)"
      ],
      "metadata": {
        "id": "cjiKtSZ_HocU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the word count of the content words\n",
        "\n",
        "len(content_words)"
      ],
      "metadata": {
        "id": "gugtaudKJKra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recalculating word frequencies using Counter\n",
        "\n",
        "content_word_counts = Counter(content_words)\n",
        "\n",
        "print(content_word_counts)"
      ],
      "metadata": {
        "id": "bnbfQejBHwDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a dataframe to store word frequency data\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame.from_dict(content_word_counts, orient=\"index\", columns=[\"count\"])\n",
        "\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "O_MPAIqhIQw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sorting the dataframe by word frequency\n",
        "\n",
        "df.sort_values(\"count\", ascending=False, inplace=True)\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "XfIJT9C1fyCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving dataframe to CSV\n",
        "\n",
        "df.to_csv(\"rapunzel_word_frequencies.csv\", header=True, index=False)"
      ],
      "metadata": {
        "id": "peduzX48_pW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercises:\n",
        "\n",
        "### Calculate and compare the relative frequency of male and female pronouns in rapunzel.txt\n",
        "\n"
      ],
      "metadata": {
        "id": "JVzD0TCMI1AK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing the text file\n",
        "\n",
        "rapunzel = open(os.path.join(grimms_folder, \"rapunzel.txt\"), encoding=\"utf-8\").read()\n",
        "\n",
        "tokens = word_tokenize(rapunzel)"
      ],
      "metadata": {
        "id": "Gvus3So5Iajj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if pronouns are stopwords or not\n",
        "\n",
        "if \"her\" in stopwords:\n",
        "  print(\"It's a stopword\")\n",
        "else:\n",
        "  print(\"It's not a stopword\")"
      ],
      "metadata": {
        "id": "poAzYifH5UKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning tokens of punctuation, and making all lowercase\n",
        "\n",
        "punctuation = list(string.punctuation)\n",
        "\n",
        "punctuation.extend([\"‘\", \"’\", \"“\", \"”\"])\n",
        "\n",
        "tokens = [token for token in tokens if token not in punctuation]\n",
        "\n",
        "tokens = [token.lower() for token in tokens]"
      ],
      "metadata": {
        "id": "0j8539aX4-H0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the number and relative frequency of female pronouns\n",
        "\n",
        "female_pronouns = [\"she\", \"her\", \"hers\", \"herself\"]\n",
        "\n",
        "female_pronoun_count = 0\n",
        "\n",
        "for x in female_pronouns:\n",
        "  for y in tokens:\n",
        "    if x == y:\n",
        "      female_pronoun_count += 1\n",
        "\n",
        "relative_female_freq = female_pronoun_count / len(tokens)\n",
        "\n",
        "print(\"The number of female pronouns is: \" + str(female_pronoun_count))\n",
        "print(\"The relative frequency of female pronouns is: \" + str(relative_female_freq))\n"
      ],
      "metadata": {
        "id": "3U_oxEDQxIwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the number and relative frequency of male pronouns\n",
        "\n",
        "male_pronouns = [\"he\", \"him\", \"his\", \"himself\"]\n",
        "\n",
        "male_pronoun_count = 0\n",
        "\n",
        "for x in male_pronouns:\n",
        "  for y in tokens:\n",
        "     if x == y:\n",
        "      male_pronoun_count += 1\n",
        "\n",
        "relative_male_freq = male_pronoun_count / len(tokens)\n",
        "\n",
        "print(\"The number of male pronouns is: \" + str(male_pronoun_count))\n",
        "print(\"The relative frequency of male pronouns is: \" + str(relative_male_freq))"
      ],
      "metadata": {
        "id": "gJpXFp9m2lBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2yJEsVWH2n4y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}